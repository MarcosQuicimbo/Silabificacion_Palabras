{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate english phrases\n",
    "fake = Faker()\n",
    "\n",
    "def generate_fake_phrase():\n",
    "    # Generate a random sentence\n",
    "    # The parameter 'nb_words' limits the number of words in the sentence\n",
    "    phrase = fake.sentence(nb_words=6)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user input for number of phrases\n",
    "def number_of_phrases():\n",
    "    number = int(input(\"How many phrases would you like to generate? \"))\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate phrases\n",
    "def generate_phases(number):\n",
    "    phrases = []\n",
    "    for i in range(number):\n",
    "        phrase = generate_fake_phrase()\n",
    "        phrases.append(phrase)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr ##ansf ##orm ##ers pr ##oj ##ect.\n"
     ]
    }
   ],
   "source": [
    "def split_into_syllables(phrases):\n",
    "    # Define a simple rule to split words into chunks\n",
    "    def split_word(word):\n",
    "        # Splitting at vowels; this is a very rough approximation\n",
    "        parts = re.findall(r'[aeiouyAEIOUY]+[^aeiouyAEIOUY]*|[^aeiouyAEIOUY]+', word)\n",
    "        if len(parts) > 1:\n",
    "            return parts[0] + ''.join(f\" ##{part}\" for part in parts[1:])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    # Split the phrase into words\n",
    "    words = phrases.split()\n",
    "    # Split each word according to the simple rule\n",
    "    split_words = [split_word(word) for word in words]\n",
    "    # Join everything back into a string\n",
    "    return ' '.join(split_words)\n",
    "\n",
    "# Example usage\n",
    "input_phrase = \"Transformers project.\"\n",
    "#input_phrase = \"Transformers project.\"\n",
    "#input_phrase = \"I am a student.\"\n",
    "\n",
    "print(split_into_syllables(input_phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers project .\n"
     ]
    }
   ],
   "source": [
    "# NO \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Function to add \"##\" to token parts\n",
    "    def add_hashes(token):\n",
    "        # This is a placeholder for more sophisticated splitting logic\n",
    "        # For now, it just identifies if there are uppercase letters indicating a new sub-token\n",
    "        parts = re.findall(r'[A-Z][^A-Z]*', token)\n",
    "        if parts:\n",
    "            return parts[0] + ''.join(f\" ##{part.lower()}\" for part in parts[1:])\n",
    "        else:\n",
    "            return token\n",
    "    \n",
    "    # Apply the custom logic to each token\n",
    "    processed_tokens = [add_hashes(token) for token in tokens]\n",
    "    \n",
    "    # Join the processed tokens back into a string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Transformers project.\"\n",
    "print(custom_tokenize(input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vida es bella\n"
     ]
    }
   ],
   "source": [
    "# NO\n",
    "def manual_split_and_hash(word):\n",
    "    # Example manual splits (very simplistic and specific to the given example)\n",
    "    splits = {\n",
    "        'Transformers': 'Trans ##for ##mers',\n",
    "        'project': 'pro ##ject',\n",
    "    }\n",
    "    return splits.get(word, word)\n",
    "\n",
    "def process_phrase(phrase):\n",
    "    # Tokenize the phrase into words and punctuation\n",
    "    tokens = word_tokenize(phrase)\n",
    "    \n",
    "    # Process each token\n",
    "    processed_tokens = [manual_split_and_hash(token) for token in tokens]\n",
    "    \n",
    "    # Reconstruct the phrase\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "input_phrase = \"La vida es bella\"\n",
    "print(process_phrase(input_phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer floor positive focus Congress attention else.\n",
      "Wr ##it ##er fl ##oor p ##os ##it ##iv ##e f ##oc ##us C ##ongr ##ess att ##ent ##ion els ##e.\n",
      "\n",
      "\n",
      "State try price test for beat team.\n",
      "St ##at ##e tr ##y pr ##ic ##e t ##est f ##or b ##eat t ##eam.\n",
      "\n",
      "\n",
      "Conference join who ago see movie.\n",
      "C ##onf ##er ##enc ##e j ##oin wh ##o ag ##o s ##ee m ##ov ##ie.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numero_de_frases = number_of_phrases()\n",
    "frases = generate_phases(numero_de_frases)\n",
    "for frase in frases:\n",
    "    print(frase)\n",
    "    print(split_into_syllables(frase))\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
